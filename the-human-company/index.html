<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta content="width=device-width,initial-scale=1" name="viewport"><title>Vincent Liu — The Human Company</title><link href="https://fonts.googleapis.com" rel="preconnect"><link href="https://fonts.gstatic.com" rel="preconnect" crossorigin=""><link href="https://fonts.googleapis.com/css2?family=Playfair+Display:ital,wght@0,400;0,700;1,400&family=Work+Sans:wght@400;500;600;700&display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/css2?family=Work+Sans:wght@400;500;600;700&display=swap" rel="stylesheet"><script>if (!Object.hasOwn) {
        Object.hasOwn = function (obj, prop) {
          return Object.prototype.hasOwnProperty.call(obj, prop);
        };
      }</script><link href="https://fonts.googleapis.com" rel="preconnect"><script crossorigin="" src="/assets/index-CWCdjH2k.js" type="module"></script><link href="/assets/react-vendor-CLvpS23b.js" rel="modulepreload" crossorigin=""><link href="/assets/index-BncA647q.css" rel="stylesheet" crossorigin=""><link href="/assets/Post-D2wZnpij.js" rel="modulepreload" crossorigin="" as="script"><meta content="On the human-centric thesis for robotics" data-rh="true" name="description"><meta content="Vincent Liu — The Human Company" data-rh="true" property="og:title"><meta content="On the human-centric thesis for robotics" data-rh="true" property="og:description"><meta content="article" data-rh="true" property="og:type"></head><body><div id="root"><div class="app-container"><nav class="mb-16"><a href="/" class="nav-brand" aria-label="home" data-discover="true" title="home">«</a></nav><main><article class="animate-in fade-in duration-700"><header class="mb-12 border-b border-stone-100 pb-8"><h1 class="text-2xl font-serif font-semibold leading-tight tracking-wide mb-4 lowercase">The Human Company</h1><div class="flex-wrap items-center gap-x-4 gap-y-2 text-stone-400"><div class="items-center gap-2 mb-2"><div class="gap-2"><span class="!text-stone-600 !text-xs">Vincent Liu, </span><span class="!text-stone-600 !text-xs">Ademi Adeniji</span></div></div><span class="metadata-text !text-stone-600 !text-xs">June 2025</span></div></header><div class="prose font-serif"><p>We exist at a precipice in human history. The last decade has seen the birth of a new class of intelligence that rivals our own cognitive capabilities as humans. One of the greatest promises of artificial general intelligence is its potential to reason about and automate tasks in the physical world, and this future of general-purpose robotics is now within reach. Our mission is to accelerate this path to abundance.</p><p>Generalization of deep neural networks scales with Internet-scale data. Both data quality and quantity are needed to produce behaviors that are correct and robust<sup><a href="#user-content-fn-1" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-1">1</a></sup>. Teleoperation, by far the most popular approach, has some unforgivable limitations—it requires trained human teleoperators, working robots, and is constrained by the set of deployable workspaces and tasks. It will be difficult to reach even 2T-token/GPT-2 scale this way<sup><a href="#user-content-fn-2" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-2">2</a></sup>, not to mention that data collected on one robot is incompatible with other morphologies. By starting from specific tasks and collecting data retroactively, teleoperation also inherently fails to reflect the true distribution of human labor.</p><p>The philosophical question should not be “how do we collect data for this task?” but rather “how do we collect data that represents the collective human experience?” Today’s powerful artificial intelligences only exist because of the Internet, which is a real-time digital reflection of the human experience. As a result, deep neural networks trained continually on the Internet will understand human culture and its evolution over time. Human data is the renewable energy of artificial intelligence and robotics.</p><p>Human foundation models trained on human data at global scale will be the only class of models that can exhibit physical intelligence at the scale of humanity. Although the human and robot embodiments are different today, the discrepancy is shrinking rapidly as humanoid hardware is becoming more commercially viable. We believe that the key to crossing this distribution gap is to bootstrap the world knowledge distilled by language and vision foundation models. We have seen a strong emergence of this trend in vision-language models<sup><a href="#user-content-fn-3" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-3">3</a></sup>, world models<sup><a href="#user-content-fn-4" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-4">4</a></sup>, and video models<sup><a href="#user-content-fn-5" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-5">5</a></sup>. As deep learning continues to scale, the emergent reasoning capabilities in human foundation models will produce general-purpose physical intelligence.</p><p>We have spent the past year developing this vision. Without raising any capital, we have developed breakthroughs in robot learning from human behavior. We have shown how to transfer in-the-wild human data from smart glasses into robot policies<sup><a href="#user-content-fn-6" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-6">6</a></sup> and tactile human data into robot policies with a sense of touch<sup><a href="#user-content-fn-7" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-7">7</a></sup>. Our results promise orders-of-magnitude gains in both sample efficiency and data collection speed over the current approaches of Physical Intelligence, Figure, Tesla, and Google Deepmind. Our method will scale as a human foundation model akin to the robot vision-language-action model<sup><a href="#user-content-fn-8" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-8">8</a></sup>, while retaining native compatibility with human data. Thus, human foundation models will scale at the rate of human data. Robot foundation models may never scale because robot data is expensive, and learning cross-embodiment invariances is even more expensive. The human foundation model still requires algorithmic innovation, but trends in other domains indicate that open-source academic research and world models will converge to an effective solution relatively quickly. In the next 2-3 years, we expect to see native human foundation models exhibit near-perfect transfer to humanoid robots.</p><p>The human-centric philosophy has advantages at both the data and model layers, but neither should be monetized. First, the model layer will be commoditized. LLMs have shown that algorithms are expensive to develop, easily copied, and trending towards $0/token rates. Second, the data layer is important but encounters several headwinds as a business model. Data monetization creates a misalignment of incentives between the laborers paid to collect data and the robotics companies buying the data. The low technical barrier to start such a company also means that competition will squeeze margins on both sides. Furthermore, the “Scale AI of robotics” is a misconception—in robotics, data and labels are inseparable, and the real cost lies in data collection rather than cheap, a-posteriori labeling. Unlike LLMs, where the data representation is standardized and static, robotics data collection and model training are tightly coupled and evolving.</p><p>And so, the one true robotics business model is to sell automation. The largest value capture will occur at the frontier of GDP growth. Monetizing automation also aligns incentives across the entire stack. People who buy automation (factory robots, house robots) will be incentivized to contribute their own data to improve their own experiences. The human foundation model will be decoupled from any robot hardware and learn continually from human data. The product will be monotonically improving automation driven by a flywheel of indefinite human data<sup><a href="#user-content-fn-9" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-9">9</a></sup>. And as goods and hardware become cheaper<sup><a href="#user-content-fn-10" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-10">10</a></sup>, economic output and margins will exponentiate.</p><p>We end this essay on a more philosophical note. Without human intelligence, there would never be silicon intelligence. We exist in a world built by humans for humans—physically, digitally, economically—so an artificial intelligence system succeeds best when it aligns with our incentives. This bleeds into all layers of the stack—data, models, product, monetization. Human data is the path to a singularity, after which robots can autonomously explore and overcome Moravec’s paradox<sup><a href="#user-content-fn-11" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-11">11</a></sup>. We are not just creating a system to scale general-purpose robots. We are building the behavioral structures that will usher humanity’s transition into abundance.</p><p><em>This essay was written by humans without any AI.</em></p><section class="footnotes" data-footnotes=""><h2 class="sr-only" id="footnote-label">Footnotes.</h2><ol><li id="user-content-fn-1"><p>Brad Porter. <a href="https://medium.com/@bp_64302/this-business-of-robotics-foundation-models-cb4bdede1444">This Business of Robotics Foundation Models.</a> <a href="#user-content-fnref-1" class="data-footnote-backref" aria-label="Back to reference 1" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-2"><p>Chris Paxton. <a href="https://itcanthink.substack.com/p/how-can-we-get-enough-data-to-train">How can we get enough data to train a robot GPT?</a> <a href="#user-content-fnref-2" class="data-footnote-backref" aria-label="Back to reference 2" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-3"><p>OpenAI. <a href="https://openai.com/index/hello-gpt-4o/">Hello GPT-4o.</a> <a href="#user-content-fnref-3" class="data-footnote-backref" aria-label="Back to reference 3" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-4"><p>Meta. <a href="https://ai.meta.com/vjepa/">Introducing V-JEPA 2: A self-supervised foundation world model.</a> <a href="#user-content-fnref-4" class="data-footnote-backref" aria-label="Back to reference 4" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-5"><p>Google Deepmind. <a href="https://deepmind.google/models/veo/">Veo: Our state-of-the-art video generation model.</a> <a href="#user-content-fnref-5" class="data-footnote-backref" aria-label="Back to reference 5" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-6"><p>Vincent Liu and Ademi Adeniji. <a href="https://egozero-robot.github.io/">EgoZero: Robot Learning from Smart Glasses.</a> <a href="#user-content-fnref-6" class="data-footnote-backref" aria-label="Back to reference 6" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-7"><p>Ademi Adeniji and Vincent Liu. <a href="https://feel-the-force-ftf.github.io/">Feel-the-Force: Contact-Driven Learning from Humans.</a> <a href="#user-content-fnref-7" class="data-footnote-backref" aria-label="Back to reference 7" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-8"><p>Physical Intelligence. <a href="https://www.physicalintelligence.company/blog/pi0">π0: Our First Generalist Policy.</a> <a href="#user-content-fnref-8" class="data-footnote-backref" aria-label="Back to reference 8" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-9"><p>Tesla. <a href="https://www.tesla.com/fsd">Full Self-Driving (Supervised).</a> <a href="#user-content-fnref-9" class="data-footnote-backref" aria-label="Back to reference 9" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-10"><p>Bain & Company. <a href="https://www.bain.com/insights/hardware-paradox-global-machinery-and-equipment-report-2022/">The Hardware Paradox: Machinery Must Expand beyond Machines.</a> <a href="#user-content-fnref-10" class="data-footnote-backref" aria-label="Back to reference 10" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-11"><p>Ege Erdil, Epoch AI. <a href="https://epoch.ai/gradient-updates/movarec-s-paradox">Moravec’s paradox and its implications.</a> <a href="#user-content-fnref-11" class="data-footnote-backref" aria-label="Back to reference 11" data-footnote-backref="">↩</a></p></li></ol></section></div></article></main><footer class="footer-section"><div class="flex gap-4 mb-4"><a href="https://twitter.com/vincentjliu" class="link-external decoration-transparent" rel="noopener noreferrer" target="_blank"><svg class="social-svg" fill="currentColor" viewBox="0 0 24 24"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"></path></svg></a><a href="https://scholar.google.com/citations?user=TY0sFqgAAAAJ" class="link-external decoration-transparent" rel="noopener noreferrer" target="_blank"><svg class="social-svg" fill="currentColor" viewBox="0 0 24 24"><path d="M12 24a7 7 0 0 1-7-7c0-2.58 1.41-4.93 3.61-6.16L5.05 7.11 12 1 18.95 7.11l-3.56 3.73C17.59 12.07 19 14.42 19 17a7 7 0 0 1-7 7zm0-12a5 5 0 1 0 0 10 5 5 0 0 0 0-10z"></path></svg></a><a href="mailto:vincent.liu15@gmail.com" class="link-external decoration-transparent" rel="noopener noreferrer" target="_blank"><svg class="social-svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path d="M3 8l7.89 5.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5"></path></svg></a></div><div class="copyright-text">© 2026 Vincent Liu</div></footer></div></div></body></html>