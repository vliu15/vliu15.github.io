<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta content="width=device-width,initial-scale=1" name="viewport"><title>Vincent Liu — The Robotics Data Pareto Frontier</title><link href="https://fonts.googleapis.com" rel="preconnect"><link href="https://fonts.gstatic.com" rel="preconnect" crossorigin=""><link href="https://fonts.googleapis.com/css2?family=Playfair+Display:ital,wght@0,400;0,700;1,400&family=Work+Sans:wght@400;500;600;700&display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/css2?family=Work+Sans:wght@400;500;600;700&display=swap" rel="stylesheet"><script>if (!Object.hasOwn) {
        Object.hasOwn = function (obj, prop) {
          return Object.prototype.hasOwnProperty.call(obj, prop);
        };
      }</script><link href="https://fonts.googleapis.com" rel="preconnect"><script crossorigin="" src="/assets/index-BJrESfhG.js" type="module"></script><link href="/assets/react-vendor-CI1IrhFw.js" rel="modulepreload" crossorigin=""><link href="/assets/index-BncA647q.css" rel="stylesheet" crossorigin=""><link href="/assets/Post-DVJHoWir.js" rel="modulepreload" crossorigin="" as="script"><meta content="On the emergence of human data in robotics (part 1)" data-rh="true" name="description"><meta content="Vincent Liu — The Robotics Data Pareto Frontier" data-rh="true" property="og:title"><meta content="On the emergence of human data in robotics (part 1)" data-rh="true" property="og:description"><meta content="article" data-rh="true" property="og:type"></head><body><div id="root"><div class="app-container"><nav class="mb-16"><a href="/" class="nav-brand" aria-label="home" data-discover="true" title="home">«</a></nav><main><article class="animate-in fade-in duration-700"><header class="mb-12 border-b border-stone-100 pb-8"><h1 class="text-2xl font-serif font-semibold leading-tight tracking-wide mb-4 lowercase">The Robotics Data Pareto Frontier</h1><div class="flex-wrap items-center gap-x-4 gap-y-2 text-stone-400"><div class="items-center gap-2 mb-2"><div class="gap-2"><span class="!text-stone-600 !text-xs">Vincent Liu</span></div></div><span class="metadata-text !text-stone-600 !text-xs">January 2026</span></div></header><div class="prose font-serif"><figure id="fig-top"><img alt="Data Pareto Frontier" src="/robotics_data_pareto_frontier-top.png" style="max-width:100%;height:auto"><figcaption>Figure 1. The real-world Data Pareto Frontier. Quality is defined as embodiment alignment, or how closely the data matches the robot's proprioceptive reality (joint angles, forces, actuation). Scalability is defined as task diversity/complexity.</figcaption></figure><p>The defining narrative of robotics in 2025 was not a new model architecture, but an enthusiasm for data. Despite a consensus around teleoperation as the gold standard, interest blossomed in the data and training recipes that transcend it. This appetite defined a new market category and spurred the creation of innumerable data collection startups aiming to address the heterogeneous robotics data problem. At the intersection of data companies and robotics research lies a recurring theme: human data. In just the last month, the industry’s largest players—whose initial hypotheses were staked on either hardware, teleoperation, or simulation—have all announced breakthroughs in learning from human data<sup><a href="#user-content-fn-1" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-1">1</a></sup> <sup><a href="#user-content-fn-2" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-2">2</a></sup> <sup><a href="#user-content-fn-3" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-3">3</a></sup>. Structurally, this resurgence is not merely a quest for scale, but an inevitability. <strong>Real-world data collection is a product, and the user experience of that product dictates a Pareto Frontier</strong> between embodiment specificity and long-tail task diversity.</p><h3>Data collection as a product</h3><p>If we define a product as a tool designed to facilitate a specific human activity at scale, then data collection should be evaluated by how it manages the exchange between human behavior and digital output. Like any product, its success is governed by a trade-off between <em>utility</em> (the value of the data) and <em>friction</em> (the burden of incentive alignment and distribution). The ideal user experience (UX) of data collection is <em>invisible</em>: the user maintains their natural workflow and the data, captured as a seamless byproduct of their labor, is effective for robot learning. The product of data collection is the bridge that tries to reconcile the robot’s need for data with the human’s need to perform their job.</p><p>Perfect data, as Sergey Levine puts it, is a robot observing itself completing a task<sup><a href="#user-content-fn-4" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-4">4</a></sup>. Teleoperation is the gold standard, providing kinematically complete data that is exactly 1-1 with the downstream embodiment. However, as a product, teleoperation has terrible UX. Popular systems such as leader-followers for bimanual manipulators or motion capture for humanoids prevent humans from completing tasks naturally in speed and quality. Furthermore, teleoperation faces high costs of hardware (you need a functioning robot), human capital (you need a trained operator), and incentive structures (you cannot ask a factory worker to remote-control a robot without disrupting their actual job). As such, teleoperation remains a lab operation in most implementations.</p><p>The barriers to scaling teleoperation have driven a large interest in handheld devices, usually in the form of portable grippers/gloves. While these devices are more portable, they decouple the hand from the robot, removing much of the kinematic information that teleoperation captures. In practice, the promise of scale falls short. For example, you cannot ask a restaurant chef to roll your specialty handmade pasta with blunt two-fingered appendages during peak operation. Consequently, handheld datasets capture breadth across tasks, but not the vast “long-tail” collection of nuanced, rare, or difficult-to-access tasks that define real-world work. Despite these limitations, handheld devices remain popular because they occupy a narrow but useful region of the Pareto curve: just enough embodiment information at just enough scale beyond teleoperation.</p><figure id="fig-mid"><img alt="Data collection examples" src="/robotics_data_pareto_frontier-mid.png" style="max-width:100%;height:auto"><figcaption>Figure 2. The main data collection categories and their tradeoffs in quality vs scalability.</figcaption></figure><p>It turns out that the incentive structures that drive data collection are governed by ergonomic (can I do my job?) and social (do I want to wear this?) parameters. Therefore, data collection methods involving proxy robot embodiments cannot capture the long-tail of physical labor because they fail the ergonomic requirement. Only a product that sufficiently achieves both criteria can collect “ego data” directly from humans while they work. Even within the category of ego data, hardware scales differently based on social factors. For example, people are more willing to wear glasses than iPhones on their heads. Though the appetite for ego data is growing rapidly, stripping away the actuators and sensors to improve UX means that we cannot capture proprioceptive ground truth.</p><p>In short, the landscape of robotics data collection can be summarized in three categories: teleoperation (high quality, low scale), handheld devices (moderate quality, moderate scale), and ego data (low quality, high scale). These represent the Data Pareto Frontier, which specifies academic taxonomies like Yuke Zhu’s Data Pyramid<sup><a href="#user-content-fn-5" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-5">5</a></sup> by observing real-world constraints. While we would like to maximize both quality and scale simultaneously, the Data Pareto Frontier represents the boundary of what reality allows us to capture. Crucially, it suggests that we should integrate along the curve to produce the optimal data collection system, and also that we should improve hardware design to push these limits.</p><h3>Closing remarks</h3><p>Ultimately, the resurgence of human data in robotics is a structural inevitability. The Data Pareto Frontier dictates that we cannot simply collect all of anything, but rather that we are bound by a tradeoff between data quality and scalability.</p><p>The solution, therefore, is not about a perfect data source, but about how to build a heterogeneous deep learning system. Robotics research labs are pivoting to human data because the Pareto Frontier is forcing them to. Scaling perfect robot data is hard, so the path forward is to slide down the curve towards ego data and hope that model scale (compression) will enable transfer. Fortunately, as Ilya Sutskever’s framework of compression<sup><a href="#user-content-fn-6" aria-describedby="footnote-label" data-footnote-ref="" id="user-content-fnref-6">6</a></sup> suggests, the long-tail messiness is not noise but actually useful information about the world that robots will profit from, and so we should be optimistic about future algorithms.</p><p>Our focus is shifting from the pursuit of a perfect data source toward building systems capable of learning across the entire curve. Success lies in designing for human adoption and improving hardware to push the Pareto Frontier.</p><p><em>I thank my good friends and peers Ademi Adeniji, Jupi Parmar, Ben Bolte, and Kartik Bhat for early feedback on this essay.</em></p><section class="footnotes" data-footnotes=""><h2 class="sr-only" id="footnote-label">Footnotes.</h2><ol><li id="user-content-fn-1"><p>1X. <a href="https://www.1x.tech/discover/world-model-self-learning">1X World Model | From Video to Action: A New Way Robots Learn</a>. <a href="#user-content-fnref-1" class="data-footnote-backref" aria-label="Back to reference 1" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-2"><p>Physical Intelligence. <a href="https://www.pi.website/research/human_to_robot">Emergence of Human to Robot Transfer in VLAs</a>. <a href="#user-content-fnref-2" class="data-footnote-backref" aria-label="Back to reference 2" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-3"><p>Skild AI. <a href="https://www.skild.ai/blogs/learning-by-watching">Learning by watching human videos</a>. <a href="#user-content-fnref-3" class="data-footnote-backref" aria-label="Back to reference 3" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-4"><p>Sergey Levine. <a href="https://sergeylevine.substack.com/p/sporks-of-agi">Sporks of AGI</a>. <a href="#user-content-fnref-4" class="data-footnote-backref" aria-label="Back to reference 4" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-5"><p>Yuke Zhu. <a href="https://www.youtube.com/watch?v=X4-Ae3Ci_-w">Building Generalist Robot Autonomy with the Data Pyramid</a>. <a href="#user-content-fnref-5" class="data-footnote-backref" aria-label="Back to reference 5" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-6"><p>Ilya Sutskever. <a href="https://www.youtube.com/watch?v=AKMuA_TVz3A">An Observation on Generalization</a>. <a href="#user-content-fnref-6" class="data-footnote-backref" aria-label="Back to reference 6" data-footnote-backref="">↩</a></p></li></ol></section></div></article></main><footer class="footer-section"><div class="flex gap-4 mb-4"><a href="https://twitter.com/vincentjliu" class="link-external decoration-transparent" rel="noopener noreferrer" target="_blank"><svg class="social-svg" fill="currentColor" viewBox="0 0 24 24"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"></path></svg></a><a href="https://scholar.google.com/citations?user=TY0sFqgAAAAJ" class="link-external decoration-transparent" rel="noopener noreferrer" target="_blank"><svg class="social-svg" fill="currentColor" viewBox="0 0 24 24"><path d="M12 24a7 7 0 0 1-7-7c0-2.58 1.41-4.93 3.61-6.16L5.05 7.11 12 1 18.95 7.11l-3.56 3.73C17.59 12.07 19 14.42 19 17a7 7 0 0 1-7 7zm0-12a5 5 0 1 0 0 10 5 5 0 0 0 0-10z"></path></svg></a><a href="mailto:vincent.liu15@gmail.com" class="link-external decoration-transparent" rel="noopener noreferrer" target="_blank"><svg class="social-svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path d="M3 8l7.89 5.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5"></path></svg></a></div><div class="copyright-text">© 2026 Vincent Liu</div></footer></div></div></body></html>